{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/search/?q=nba&sort=new\n",
      "No more clickable buttons. Exiting the loop.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 213\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA função alinea_b retornou uma string inválida.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;66;03m# categoria=f_categoria('valein')\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;66;03m# carregar_mais_posts(3,categoria)\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# algoritmo_c(3,'valein')\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m \u001b[43malinea_b\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnba\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m################################ Joana\u001b[39;00m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbs4\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n",
      "Cell \u001b[1;32mIn[1], line 125\u001b[0m, in \u001b[0;36malinea_b\u001b[1;34m(limite, categoria)\u001b[0m\n\u001b[0;32m    123\u001b[0m     comentarios\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,comentarios)\n\u001b[0;32m    124\u001b[0m     url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.reddit.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomentarios\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 125\u001b[0m     lista,score\u001b[38;5;241m=\u001b[39m\u001b[43mcomet\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[0;32m    127\u001b[0m     lista\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 66\u001b[0m, in \u001b[0;36mcomet\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     64\u001b[0m lista_comentarios\u001b[38;5;241m=\u001b[39m[]\n\u001b[0;32m     65\u001b[0m soup\u001b[38;5;241m=\u001b[39mcarrega_mais_comentarios(url)\n\u001b[1;32m---> 66\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshreddit-post\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock xs:mt-xs xs:-mx-xs xs:px-xs xs:rounded-[16px] pt-xs nd:pt-xs bg-[color:var(--shreddit-content-background)] box-border mb-xs nd:visible nd:pb-2xl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     67\u001b[0m score\u001b[38;5;241m=\u001b[39mre\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.*?\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;28mstr\u001b[39m(x))\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(score) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import\n",
    "url = 'https://www.reddit.com/r/sportsgambling/comments/18utbdo/nba_100_hit_rate_sheets_for_last_3_5_games/'\n",
    "\n",
    "# Use the requests library to fetch the page content\n",
    "response = requests.get(url)\n",
    "html = response.text\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Now you can work with the soup object to extract information from the page\n",
    "# For example, print the title of the page\n",
    "\n",
    "todos = soup.find_all('shreddit-comments-page-ad')\n",
    "print(todos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algoritmo_c(limite, categoria):\n",
    "    resultado_alinea_b = alinea_b(limite,categoria)\n",
    "    try:\n",
    "        resultado_alinea_b = json.loads(resultado_alinea_b)\n",
    "        if resultado_alinea_b:\n",
    "            for post in range(len(resultado_alinea_b)):\n",
    "                score = resultado_alinea_b[post].get('score', 'N/A') # score\n",
    "                comment=resultado_alinea_b[post].get('comments', 'N/A')\n",
    "                n_comment=len(comment) # num de comentarios\n",
    "                num_resposta=0\n",
    "                for k in comment:\n",
    "                    resposta = k.get('resposta')\n",
    "                    if resposta:\n",
    "                        num_resposta+=len(resposta) # quantas respostas para cada comentario\n",
    "                resultado_alinea_b[post]['relevancia']= 0.5*int(score)+0.4*n_comment+0.1*num_resposta\n",
    "            json_relevancia =sorted(resultado_alinea_b, key=lambda x: x.get('relevancia', 0), reverse=True)\n",
    "            json_relevancia=json.dumps(json_relevancia, indent=2)\n",
    "            print(json_relevancia)\n",
    "\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"A função alinea_b retornou uma string inválida.\")\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "#     qst=str(input(f'Deseja comparar os resultados com os mais populares da categoria {categoria} ?'))\n",
    "\n",
    "#     while qst != 's' and qst != 'n':\n",
    "#         qst=str(input('Insira (s) para sim ou (n) para não, em minúsculo: '))\n",
    "\n",
    "#     if qst == 's':\n",
    "#         alinea_d(limite, categoria)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def alinea_d(limite, categoria):\n",
    "#     url_d = f'https://www.reddit.com/search/?q={categoria}&sort=hot'\n",
    "#     posts_cat = carregar_mais_posts(limite, url_d)\n",
    "    \n",
    "\n",
    "\n",
    "# categoria=f_categoria('valeim')\n",
    "# carregar_mais_posts(3,categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/search/?q=nba&sort=new\n",
      "https://www.reddit.com/search/?q=nba&sort=new\n",
      "/r/nba/comments/18wc4ka/highlight_kat_called_for_charge_via_the_rare/\n",
      "No more clickable buttons. Exiting the loop.\n",
      "/r/nba/comments/18wc1dy/rankin_frank_vogel_also_said_bol_bol_will_see/\n",
      "No more clickable buttons. Exiting the loop.\n",
      "Element 'faceplate-batch' not found.\n",
      "/r/ripcity/comments/18wbzp3/game_thread_the_portland_trail_blazers_922_the/\n",
      "No more clickable buttons. Exiting the loop.\n",
      "{\n",
      "  \"post 1\": {\n",
      "    \"title\": \"[Highlight] KAT called for charge via the rare transitive property of offensive fouls\",\n",
      "    \"subreddit\": \"/r/nba\",\n",
      "    \"comments\": [\n",
      "      {\n",
      "        \"autor\": \"smkmn13\",\n",
      "        \"comentario\": \"Note: Call was challenged and upheld as \\\"marginal contact to Grimes\\\"\",\n",
      "        \"score\": \"1\"\n",
      "      }\n",
      "    ],\n",
      "    \"score\": \"0\",\n",
      "    \"time\": \"2024:01:02 00:57:21\"\n",
      "  },\n",
      "  \"post 2\": {\n",
      "    \"title\": \"[Rankin] Frank Vogel also said Bol Bol will see action tonight. #Suns\",\n",
      "    \"subreddit\": \"/r/nba\",\n",
      "    \"comments\": [],\n",
      "    \"score\": \"7\",\n",
      "    \"time\": \"2024:01:02 00:53:16\"\n",
      "  },\n",
      "  \"post 3\": {\n",
      "    \"title\": \"GAME THREAD: The Portland Trail Blazers (9-22) @ The Phoenix Suns (17-15) - (6:00 PM PT, Monday, January, 2024)\",\n",
      "    \"subreddit\": \"/r/ripcity\",\n",
      "    \"comments\": [\n",
      "      {\n",
      "        \"autor\": \"Kazekid\",\n",
      "        \"comentario\": \"Looks like Sharpe is a go tonight but Camara is not\",\n",
      "        \"score\": \"1\"\n",
      "      },\n",
      "      {\n",
      "        \"autor\": \"Kazekid\",\n",
      "        \"comentario\": \"KD has been on a terror lately so I was fully expecting him to come out and try to destroy us after we won last game. Thankfully, they are resting him on a b2b.\",\n",
      "        \"score\": \"1\"\n",
      "      }\n",
      "    ],\n",
      "    \"score\": \"2\",\n",
      "    \"time\": \"2024:01:02 00:50:59\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import json\n",
    "from difflib import get_close_matches\n",
    "import typer \n",
    "from urllib.parse import urljoin\n",
    "\n",
    "#confere se já existe um arquivo csv com o nome escolhido e direciona para a função escrever_csv\n",
    "def csv_(dic):\n",
    "     print(dic)\n",
    "     nome_arquivo_csv = str(input('Qual será o nome do arquivo? ')) + '.csv' #input nome.csv\n",
    "     if arquivo_existe(nome_arquivo_csv): #chama a função que verifica a existência dos arquivos\n",
    "            novo_csv=str(input(f\"O arquivo '{nome_arquivo_csv}' já existe. Deseja criar outro?(s/n): \"))\n",
    "            while novo_csv != 's' and novo_csv != 'n':\n",
    "                novo_csv=str(input('Insira (s) para sim ou (n) para não, minúsculo: '))\n",
    "            if novo_csv == 's':\n",
    "                nome_arquivo_csv = str(input('Qual será o nome do arquivo? ')) + '.csv' \n",
    "                while arquivo_existe(nome_arquivo_csv):\n",
    "                     nome_arquivo_csv = str(input('Arquivo existente.Insira outro nome: ')) + '.csv'\n",
    "                escrever_csv(dic,nome_arquivo_csv,'w') #funçao que cria csv\n",
    "            elif novo_csv == 'n':\n",
    "                 print('Será adicionado ao arquivo existente'.upper())\n",
    "                 escrever_csv(dic,nome_arquivo_csv,'a') # função que adiciona csv\n",
    "     else:\n",
    "          escrever_csv(dic,nome_arquivo_csv,'w')\n",
    "\n",
    "# função que cria um novo csv ou escreve num existente\n",
    "def escrever_csv(dic,nome_arquivo_csv, tipo):\n",
    "    with open(nome_arquivo_csv, tipo, encoding='utf-8') as arquivo_csv:\n",
    "        if tipo=='w':\n",
    "            headers=','.join(dic[1].keys())\n",
    "            arquivo_csv.write(f'{headers}\\n')\n",
    "        for post in dic:\n",
    "            values=','.join(str(value) for value in dic[post].values())\n",
    "            arquivo_csv.write(f'{values}\\n')\n",
    "\n",
    "#função que verifica os nomes dos csvs na pasta      \n",
    "def arquivo_existe(nome_arquivo):\n",
    "    return os.path.exists(nome_arquivo)\n",
    "\n",
    "def carregar_mais_posts(n_posts, url):\n",
    "    # Configurando o WebDriver (certifique-se de ter o chromedriver ou geckodriver instalado)\n",
    "    print(url)\n",
    "    soup = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome()  # ou webdriver.Firefox()\n",
    "        driver.get(url)\n",
    "        n_interacoes = 1\n",
    "        while n_interacoes < 5:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            z = soup.find_all('a', class_='absolute inset-0')\n",
    "            if len(z) == 0:\n",
    "                tudo=soup.find('shreddit-feed', class_='nd:visible')\n",
    "                z=tudo.find_all('article', class_='m-0')\n",
    "            in_n_posts = len(z)\n",
    "            time.sleep(5)\n",
    "            if in_n_posts < n_posts:\n",
    "                n_interacoes += 1\n",
    "            else: break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally: \n",
    "        driver.quit()\n",
    "    return soup\n",
    "\n",
    "\n",
    "def carrega_mais_comentarios(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    n_interacoes = 7\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    " # assegurar que começa com valor antes \n",
    "    while n_interacoes > 0:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        try:\n",
    "            load_more_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"comment-tree\"]/faceplate-partial/div[1]/button')))\n",
    "            load_more_button.click()\n",
    "        except TimeoutException:\n",
    "            print(\"No more clickable buttons. Exiting the loop.\")\n",
    "            break\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        time.sleep(1)\n",
    "        n_interacoes -= 1\n",
    "    driver.quit()\n",
    "    return soup\n",
    "\n",
    "def info_comment(coment, lista,include_score = True): # PARA CADA COMENTÁRIO INDIVIDUAL\n",
    "    autor = coment.find('div', class_='flex flex-row items-center overflow-hidden').text.strip()\n",
    "    # print(autor)\n",
    "    if autor != '[deleted]' :\n",
    "        dici = {}\n",
    "        texto = coment.find('div', slot='comment')\n",
    "        if texto is not None:\n",
    "            texto = texto.text.strip()\n",
    "        dici['autor'] = autor\n",
    "        dici['comentario'] = texto\n",
    "        resposta = coment.find('div', slot='children')\n",
    "        # print(resposta)\n",
    "        if resposta is not None:\n",
    "            lista_resposta = response_comment(resposta.find_all('div', id='-post-rtjson-content'))\n",
    "            dici['resposta'] = lista_resposta\n",
    "        if include_score:\n",
    "            sc=coment.find('shreddit-comment-action-row')\n",
    "            score=sc['score']\n",
    "            dici['score']=score\n",
    "            # print(score)\n",
    "        lista.append(dici)\n",
    "    \n",
    "def response_comment(resposta_texto, include_score = False):  # ? FAZER O SCORE PARA RESPOSTAS ?\n",
    "    lista_resposta = []\n",
    "    for c in resposta_texto:\n",
    "        lista_resposta.append(c.text.strip())\n",
    "    return lista_resposta\n",
    "\n",
    "\n",
    "def extract_comments(url):\n",
    "    lista_comentarios = []\n",
    "    soup = carrega_mais_comentarios(url)\n",
    "    if soup is not None:\n",
    "        x = soup.find('shreddit-post', class_='block xs:mt-xs xs:-mx-xs xs:px-xs xs:rounded-[16px] pt-xs nd:pt-xs bg-[color:var(--shreddit-content-background)] box-border mb-xs nd:visible nd:pb-2xl')\n",
    "        if x is not None:\n",
    "            score = re.findall(r'score=\".*?\"', str(x))\n",
    "            score = re.sub(r'score=|\"', '', score[0]) if score else None\n",
    "            todos = soup.find('faceplate-batch', target='#comment-tree') # para comentarios\n",
    "            if todos is not None:\n",
    "                cada = todos.find_all('shreddit-comment', class_='pt-md px-md xs:px-0')\n",
    "                if cada:\n",
    "                    for coment in cada:\n",
    "                        info_comment(coment, lista_comentarios)\n",
    "                    return lista_comentarios, score\n",
    "                else:\n",
    "                    print(\"No 'shreddit-comment' elements found.\")\n",
    "                    return [], score\n",
    "            else:\n",
    "                print(\"Não existe comentários para o post\")\n",
    "                return [], score\n",
    "        else:\n",
    "            print(\"Element 'shreddit-post' not found.\")\n",
    "            return [], None\n",
    "    else:\n",
    "        print(\"Soup object is None. Unable to proceed.\")\n",
    "        return [], None\n",
    "\n",
    "def f_categoria(categoria):\n",
    "    lista_cat=[]\n",
    "    urlx='https://www.reddit.com/'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(urlx)\n",
    "    time.sleep(4)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.quit()\n",
    "    topicos=soup.find('faceplate-auto-height-animator', class_='block')\n",
    "    cada=topicos.find_all('left-nav-topic-tracker', noun='topic_item')\n",
    "    for i in cada:\n",
    "        t=i.find('span', class_='flex flex-col justify-center min-w-0 shrink py-[var(--rem6)]')\n",
    "        t=t.text.strip()\n",
    "        lista_cat.append(t.lower())\n",
    "    palavra_proxima = get_close_matches(categoria.lower(), lista_cat)\n",
    "    if palavra_proxima:\n",
    "        topico=palavra_proxima[0]\n",
    "        cat=f'https://www.reddit.com/search/?q={topico}&sort=new'\n",
    "        print(cat)\n",
    "        return cat\n",
    "    else:\n",
    "        return None \n",
    "\n",
    "\n",
    "# função que pega n posts mais populares da categoria popular (alinea a: (top(N)) ou e: (top(N,include_score=True)) ) ou de uma dada categoria (uso na alinea d)\n",
    "def top(num: int, include_score = False, url_d = 'https://www.reddit.com/r/popular/'):\n",
    "    top={}\n",
    "    soup = carregar_mais_posts(20, url_d) \n",
    "    if soup:\n",
    "        n=0\n",
    "        # if len(posts_ind)==num: #mudar para < depois\n",
    "        #     print('menor')\n",
    "        # else:\n",
    "\n",
    "        if url_d == 'https://www.reddit.com/r/popular/':\n",
    "            posts=soup.find('shreddit-feed',class_='nd:visible')\n",
    "            posts_ind=posts.find_all('article',class_=\"m-0\")\n",
    "\n",
    "            for p in posts_ind:\n",
    "                if n<num:\n",
    "                    n+=1\n",
    "                    info=p.find('shreddit-post')\n",
    "                    titulo=info['post-title']\n",
    "                    subreddit=info['subreddit-prefixed-name']\n",
    "                    score=info['score']\n",
    "                    coment=info['comment-count']\n",
    "                    if include_score == True:\n",
    "                        pl = p.find('a', slot='full-post-link')\n",
    "                        post_link = pl['href']\n",
    "                        url = get_url_post(post_link)\n",
    "                        # print(url)\n",
    "                        comment_w_score, scor = extract_comments(url)\n",
    "                        # print(comment_w_score, scor)\n",
    "                        if comment_w_score != []:\n",
    "                            sorted_list = sorted(comment_w_score, key=lambda x: int(x['score']), reverse=True)\n",
    "                            sort_n = sorted_list[:5]\n",
    "                            # sort_n = json.dumps(sorted_list,ensure_ascii=False, indent=2)\n",
    "                            top[n]={'titulo':titulo,\n",
    "                                'subreddit':subreddit,\n",
    "                                'score POST':score,\n",
    "                                'comment':sort_n,\n",
    "                                }\n",
    "                        else:\n",
    "                            top[n]={'titulo':titulo,\n",
    "                                'subreddit':subreddit,\n",
    "                                'score POST':score,\n",
    "                                'comment': 'indisponível',\n",
    "                                }\n",
    "                    else: \n",
    "                        top[n]={'titulo':titulo,\n",
    "                            'subreddit':subreddit,\n",
    "                            'score POST':score,\n",
    "                            'comment':coment}\n",
    "            \n",
    "        else: \n",
    "            posts=soup.find('main')\n",
    "            posts_ind=posts.find_all('post-consume-tracker')\n",
    "            for p in posts_ind:\n",
    "                n+=1\n",
    "                if n<=num:\n",
    "                    ttl=p.find('a', class_='absolute inset-0')\n",
    "                    # print(info)\n",
    "                    titulo=(ttl.find('span')).text\n",
    "                    sbr=p.find('a', class_='flex items-center text-neutral-content-weak font-semibold')\n",
    "                    subreddit=sbr.text           \n",
    "                    # print(subbredit)         \n",
    "                    inter=p.find('div', class_='text-neutral-content-weak text-12')\n",
    "                    span=inter.find_all('faceplate-number')\n",
    "                    score=span[0]['number']\n",
    "                    coment=span[1]['number']\n",
    "                    if include_score == True:\n",
    "                        post_link = ttl['href']\n",
    "                        get_url_post(post_link)\n",
    "                        # print(url)\n",
    "                        comment_w_score, scor = extract_comments(url)\n",
    "                        if comment_w_score != []:\n",
    "\n",
    "                            sorted_list = sorted(comment_w_score, key=lambda x: int(x['score']), reverse=True)\n",
    "                            sort_n = sorted_list[:5]\n",
    "                            # sort_n = json.dumps(sorted_list,ensure_ascii=False, indent=2)\n",
    "                            top[n]={'titulo':titulo,\n",
    "                                'subreddit':subreddit,\n",
    "                                'score POST':score,\n",
    "                                'comment':sort_n,\n",
    "                                }\n",
    "                        else:\n",
    "                            top[n]={'titulo':titulo,\n",
    "                                'subreddit':subreddit,\n",
    "                                'score POST':score,\n",
    "                                'comment': 'indisponível',\n",
    "                                }\n",
    "                    else: \n",
    "                        top[n]={'titulo':titulo,\n",
    "                            'subreddit':subreddit,\n",
    "                            'score POST':score,\n",
    "                            'comment':coment}\n",
    "            \n",
    "        print(json.dumps(top,ensure_ascii=False, indent=2))\n",
    "        csv=str(input('Deseja inportar para formato csv(s/n)? '))\n",
    "        while csv != 's' and csv != 'n':\n",
    "            csv=str(input('Insira (s) para sim ou (n) para não, minúsculo: '))\n",
    "        if csv == 's':\n",
    "            csv_(top)\n",
    "    else:\n",
    "        print(f'Falha ao obter a página.')\n",
    "    return json.dumps(top,ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "def alinea_b(limite, categoria):\n",
    "    categoria = f_categoria(categoria)\n",
    "    if categoria is not None:\n",
    "        dict_b = {}\n",
    "        soup = carregar_mais_posts(limite, categoria)\n",
    "        if soup is not None:\n",
    "            z = soup.find_all('a', class_='absolute inset-0')\n",
    "            times = soup.find_all('span', class_='flex items-center text-neutral-content-weak text-12')\n",
    "            n = 0\n",
    "            for i in range(limite):\n",
    "                dici_post = {}\n",
    "                titulo = re.findall(r'aria-label=\".*?\"', str(z[i]))\n",
    "                if len(titulo) > 0:\n",
    "                    titulo = titulo[0]\n",
    "                    titulo = re.sub(r'aria-label=', '', titulo)\n",
    "                    titulo = re.sub(r'\"', '', titulo)\n",
    "                subredit = re.findall(r'href=\".*?/comments/', str(z[i]))\n",
    "                if len(subredit) > 0:\n",
    "                    subredit = subredit[0]\n",
    "                    subredit = re.sub(r'href=\"', '', subredit)\n",
    "                    subredit = re.sub(r'/comments/', '', subredit)\n",
    "                comentarios = re.findall(r'href=\".*?\"', str(z[i]))\n",
    "                if len(comentarios) > 0:\n",
    "                    comentarios = comentarios[0]\n",
    "                    comentarios = re.sub(r'href=', '', comentarios)\n",
    "                    comentarios = re.sub(r'\"', '', comentarios)\n",
    "                    print(comentarios)\n",
    "                    url = f'https://www.reddit.com{comentarios}'\n",
    "                    lista, score = extract_comments(url)\n",
    "                else:\n",
    "                    lista = None\n",
    "                    score = None\n",
    "                tempo = re.findall(r'faceplate-timeago ts=\".*?\"', str(times[i]))\n",
    "                if len(tempo) > 0:\n",
    "                    tempo = tempo[0]\n",
    "                    tempo = re.sub(r'faceplate-timeago ts=', '', tempo)\n",
    "                    tempo = re.sub(r'\"', '', tempo)\n",
    "                    data_hora_objeto = datetime.strptime(tempo, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "                    data_hora_formatada = data_hora_objeto.strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "                else:\n",
    "                    data_hora_formatada = None\n",
    "                dici_post['title'] = titulo\n",
    "                dici_post['subreddit'] = subredit\n",
    "                dici_post['comments'] = lista\n",
    "                dici_post['score'] = score\n",
    "                dici_post['time'] = data_hora_formatada\n",
    "                dict_b[f'post {i + 1}'] = dici_post  # Use a key like 'post_1', 'post_2', etc.\n",
    "            json_string = json.dumps(dict_b, ensure_ascii=False, indent=2)\n",
    "            # print(json_string)\n",
    "            return json_string\n",
    "        else:\n",
    "            print('Tópico não encontrado')\n",
    "            return None\n",
    "    else:\n",
    "        print('Soup Is NONE.')\n",
    "\n",
    "def algoritmo_c(limite, categoria):\n",
    "    resultado_alinea_b = alinea_b(limite, categoria)\n",
    "    try:\n",
    "        resultado_alinea_b = json.loads(resultado_alinea_b)\n",
    "        if resultado_alinea_b:\n",
    "            data_atual = datetime.now()\n",
    "            for post in resultado_alinea_b.keys():  # Iterate over dictionary keys\n",
    "                post_data = resultado_alinea_b[post]\n",
    "                score = post_data.get('score', 'N/A')  # score\n",
    "                comment = post_data.get('comments', 'N/A')\n",
    "                time = post_data.get('time', 'N/A')\n",
    "                if time is not None:\n",
    "                    time = datetime.strptime(time, '%Y:%m:%d %H:%M:%S')\n",
    "                    dif_tempo = abs((data_atual - time).total_seconds())\n",
    "                else:\n",
    "                    dif_tempo = 100000000000000000\n",
    "\n",
    "                n_comment = len(comment)  # num de comentarios\n",
    "                num_resposta = 0\n",
    "                if n_comment > 0:\n",
    "                    for k in comment:\n",
    "                        if isinstance(k, dict):  # Check if it's a dictionary\n",
    "                            resposta = k.get('resposta')\n",
    "                            if resposta:\n",
    "                                num_resposta += len(resposta)  # quantas respostas para cada comentario\n",
    "\n",
    "                resultado_alinea_b[post]['relevancia'] = (0.5 * int(score) if score is not None else 0 +\n",
    "                                                          0.4 * n_comment + 0.1 * num_resposta) + 100\n",
    "\n",
    "                while dif_tempo > 60:  # mais de 1 minuto\n",
    "                    dif_tempo = dif_tempo - 60\n",
    "                    resultado_alinea_b[post]['relevancia'] = resultado_alinea_b[post]['relevancia'] - 1\n",
    "\n",
    "            # Sorting the dictionary by 'relevancia'\n",
    "            sorted_dict = dict(sorted(resultado_alinea_b.items(), key=lambda x: x[1].get('relevancia', 0), reverse=True))\n",
    "\n",
    "            json_relevancia = json.dumps(sorted_dict, ensure_ascii=False, indent=2)\n",
    "            # print(json_relevancia)\n",
    "    except json.JSONDecodeError:\n",
    "        print('Error decoding JSON.')\n",
    "        return None\n",
    "\n",
    "\n",
    "    qst=str(input(f'Deseja comparar os resultados com os mais populares da categoria {categoria} ?'))\n",
    "\n",
    "    while qst != 's' and qst != 'n':\n",
    "        qst=str(input('Insira (s) para sim ou (n) para não, em minúsculo: '))\n",
    "\n",
    "    if qst == 's':\n",
    "        populares_categ=alinea_d(limite, categoria)\n",
    "        dict1=json.loads(json_relevancia)\n",
    "        dict2= json.loads(populares_categ)\n",
    "        # print(dict2)\n",
    "        # print(dict1)\n",
    "        compal = {\n",
    "        'RELEVÂNCIAS': dict1,\n",
    "        'POPULARES CATEGORIA': dict2\n",
    "    }\n",
    "        print(json.dumps(compal,indent=2))\n",
    "\n",
    "def get_url_post(href):\n",
    "    base_url = \"https://www.reddit.com\"\n",
    "    if not re.match(r'^https://www\\.reddit\\.com', href):                            \n",
    "        url = urljoin(base_url, href)\n",
    "    else:\n",
    "        url = href\n",
    "    return url\n",
    "\n",
    "# função que é usada como opção para comparar posts mais relevantes com posts mais recentes a partir da função algoritmo_c\n",
    "def alinea_d(limite, categoria):\n",
    "    d = f'https://www.reddit.com/search/?q={categoria}&sort=hot'\n",
    "    # posts_cat = carregar_mais_posts(limite, url_d)\n",
    "    # print(posts_cat)\n",
    "    resk = top(limite, url_d = d)\n",
    "    # print(resk)\n",
    "    return resk\n",
    "\n",
    "# alinea_d(3,'nba')\n",
    "# top(3) # quantos coms\n",
    "# alinea_b(3,'nba') # tem os coms\n",
    "algoritmo_c(2, 'nba') # comprimento dos coms mas actly tem os coms\n",
    "# top(3, include_score=True)\n",
    "# #f_categoria('nba')\n",
    "\n",
    "\n",
    "#criação de aplicativo de linha de comando\n",
    "app=typer.Typer()\n",
    "#para que o cliente use a função\n",
    "# @app.command()\n",
    "\n",
    "\n",
    "# def teste(nome: str):\n",
    "#     print(nome)\n",
    "\n",
    "# #correr o typer\n",
    "# if __name__=='__main__':\n",
    "#     app()\n",
    "\n",
    "\n",
    "\n",
    "# response = requests.get('https://www.reddit.com/r/Arcade1Up/comments/18vmfyh/nba_jam_xl_521_on_target_should_i_go_tor_it/')\n",
    "# html = response.text\n",
    "\n",
    "# # Parse the HTML with BeautifulSoup\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# # Now you can work with the soup object to extract information from the page\n",
    "# # For example, print the title of the page\n",
    "# print(soup.find(\"shreddit-comment\")) #todos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
