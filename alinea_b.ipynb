{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re\n",
    "from datetime import datetime\n",
    "import re\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import json\n",
    "from difflib import get_close_matches\n",
    "import typer \n",
    "\n",
    "#confere se já existe um arquivo csv com o nome escolhido e direciona para a função escrever_csv\n",
    "def csv_(dic):\n",
    "     print(dic)\n",
    "     nome_arquivo_csv = str(input('Qual será o nome do arquivo? ')) + '.csv' #input nome.csv\n",
    "     if arquivo_existe(nome_arquivo_csv): #chama a função que verifica a existência dos arquivos\n",
    "            novo_csv=str(input(f\"O arquivo '{nome_arquivo_csv}' já existe. Deseja criar outro?(s/n): \"))\n",
    "            while novo_csv != 's' and novo_csv != 'n':\n",
    "                novo_csv=str(input('Insira (s) para sim ou (n) para não, minúsculo: '))\n",
    "            if novo_csv == 's':\n",
    "                nome_arquivo_csv = str(input('Qual será o nome do arquivo? ')) + '.csv' \n",
    "                while arquivo_existe(nome_arquivo_csv):\n",
    "                     nome_arquivo_csv = str(input('Arquivo existente.Insira outro nome: ')) + '.csv'\n",
    "                escrever_csv(dic,nome_arquivo_csv,'w') #funçao que cria csv\n",
    "            elif novo_csv == 'n':\n",
    "                 print('Será adicionado ao arquivo existente'.upper())\n",
    "                 escrever_csv(dic,nome_arquivo_csv,'a') # função que adiciona csv\n",
    "     else:\n",
    "          escrever_csv(dic,nome_arquivo_csv,'w')\n",
    "\n",
    "# função que cria um novo csv ou escreve num existente\n",
    "def escrever_csv(dic,nome_arquivo_csv, tipo):\n",
    "    with open(nome_arquivo_csv, tipo, encoding='utf-8') as arquivo_csv:\n",
    "        if tipo=='w':\n",
    "            headers=','.join(dic[1].keys())\n",
    "            arquivo_csv.write(f'{headers}\\n')\n",
    "        for post in dic:\n",
    "            values=','.join(str(value) for value in dic[post].values())\n",
    "            arquivo_csv.write(f'{values}\\n')\n",
    "\n",
    "#função que verifica os nomes dos csvs na pasta      \n",
    "def arquivo_existe(nome_arquivo):\n",
    "    return os.path.exists(nome_arquivo)\n",
    "\n",
    "def carregar_mais_posts(n_posts, url):\n",
    "    # Configurando o WebDriver (certifique-se de ter o chromedriver ou geckodriver instalado)\n",
    "    print(url)\n",
    "    soup = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome()  # ou webdriver.Firefox()\n",
    "        driver.get(url)\n",
    "        n_interacoes = 1\n",
    "        while n_interacoes < 5:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            html = driver.page_source\n",
    "            soup = BeautifulSoup(html, 'html.parser')\n",
    "            z = soup.find_all('a', class_='absolute inset-0')\n",
    "            if len(z) == 0:\n",
    "                tudo=soup.find('shreddit-feed', class_='nd:visible')\n",
    "                z=tudo.find_all('article', class_='m-0')\n",
    "            in_n_posts = len(z)\n",
    "            time.sleep(5)\n",
    "            if in_n_posts < n_posts:\n",
    "                n_interacoes += 1\n",
    "            else: break\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally: \n",
    "        driver.quit()\n",
    "    return soup\n",
    "\n",
    "\n",
    "\n",
    "def carrega_mais_comentarios(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    n_interacoes = 7\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    " # assegurar que começa com valor antes \n",
    "    while n_interacoes > 0:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        try:\n",
    "            load_more_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"comment-tree\"]/faceplate-partial/div[1]/button')))\n",
    "            load_more_button.click()\n",
    "        except TimeoutException:\n",
    "            print(\"No more clickable buttons. Exiting the loop.\")\n",
    "            break\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        time.sleep(1)\n",
    "        n_interacoes -= 1\n",
    "    driver.quit()\n",
    "    return soup\n",
    "\n",
    "# print(carrega_mais_comentarios('https://www.reddit.com/r/portugal/comments/18udyh9/director_da_emel_ameaca_developers_que_fizeram/'))\n",
    "\n",
    "def info_comment(coment, lista,include_score = True): # PARA CADA COMENTÁRIO INDIVIDUAL\n",
    "    autor = coment.find('div', class_='flex flex-row items-center overflow-hidden').text.strip()\n",
    "    # print(autor)\n",
    "    if autor != '[deleted]' :\n",
    "        dici = {}\n",
    "        texto = coment.find('div', slot='comment')\n",
    "        if texto is not None:\n",
    "            texto = texto.text.strip()\n",
    "        dici['autor'] = autor\n",
    "        dici['comentario'] = texto\n",
    "        resposta = coment.find('div', slot='children')\n",
    "        # print(resposta)\n",
    "        if resposta is not None:\n",
    "            lista_resposta = response_comment(resposta.find_all('div', id='-post-rtjson-content'))\n",
    "            dici['resposta'] = lista_resposta\n",
    "        if include_score:\n",
    "            sc=coment.find('shreddit-comment-action-row')\n",
    "            score=sc['score']\n",
    "            dici['score']=score\n",
    "            # print(score)\n",
    "        lista.append(dici)\n",
    "    \n",
    "def response_comment(resposta_texto, include_score = False):  # ? FAZER O SCORE PARA RESPOSTAS ?\n",
    "    lista_resposta = []\n",
    "    for c in resposta_texto:\n",
    "        lista_resposta.append(c.text.strip())\n",
    "    return lista_resposta\n",
    "\n",
    "\n",
    "def extract_comments(url):\n",
    "    lista_comentarios = []\n",
    "    soup = carrega_mais_comentarios(url)\n",
    "    if soup is not None:\n",
    "        x = soup.find('shreddit-post', class_='block xs:mt-xs xs:-mx-xs xs:px-xs xs:rounded-[16px] pt-xs nd:pt-xs bg-[color:var(--shreddit-content-background)] box-border mb-xs nd:visible nd:pb-2xl')\n",
    "        if x is not None:\n",
    "            score = re.findall(r'score=\".*?\"', str(x))\n",
    "            score = re.sub(r'score=|\"', '', score[0]) if score else None\n",
    "            todos = soup.find('faceplate-batch', target='#comment-tree')\n",
    "            if todos is not None:\n",
    "                cada = todos.find_all('shreddit-comment', class_='pt-md px-md xs:px-0')\n",
    "                if cada:\n",
    "                    for coment in cada:\n",
    "                        info_comment(coment, lista_comentarios)\n",
    "                    return lista_comentarios, score\n",
    "                else:\n",
    "                    print(\"No 'shreddit-comment' elements found.\")\n",
    "                    return [], None\n",
    "            else:\n",
    "                print(\"Element 'faceplate-batch' not found.\")\n",
    "                return [], None\n",
    "        else:\n",
    "            print(\"Element 'shreddit-post' not found.\")\n",
    "            return [], None\n",
    "    else:\n",
    "        print(\"Soup object is None. Unable to proceed.\")\n",
    "        return [], None\n",
    "    \n",
    "\n",
    "def alinea_b (limite,categoria):\n",
    "    categoria=f_categoria(categoria)\n",
    "    if categoria is not None:\n",
    "        lista_b=[]\n",
    "        soup=carregar_mais_posts(limite,categoria)\n",
    "        if soup is not None:\n",
    "            z = soup.find_all('a', class_='absolute inset-0')\n",
    "            times=soup.find_all('span', class_='flex items-center text-neutral-content-weak text-12')\n",
    "            n=0\n",
    "            for i in range(limite):\n",
    "                dici_post={}\n",
    "                titulo=re.findall(r'aria-label=\".*?\"',str(z[i]))\n",
    "                if len(titulo)>0:\n",
    "                    titulo=titulo[0]\n",
    "                    titulo=re.sub(r'aria-label=','',titulo)\n",
    "                    titulo=re.sub(r'\"','',titulo)\n",
    "                subredit=re.findall(r'href=\".*?/comments/',str(z[i]))\n",
    "                if len(subredit)>0:\n",
    "                    subredit=subredit[0]\n",
    "                    subredit=re.sub(r'href=\"','',subredit)\n",
    "                    subredit=re.sub(r'/comments/','',subredit)\n",
    "                comentarios=re.findall(r'href=\".*?\"',str(z[i]))\n",
    "                if len(comentarios)>0:\n",
    "                    comentarios=comentarios[0]\n",
    "                    comentarios=re.sub(r'href=','',comentarios)\n",
    "                    comentarios=re.sub(r'\"','',comentarios)\n",
    "                    url=f'https://www.reddit.com{comentarios}'\n",
    "                    lista,score=extract_comments(url)\n",
    "                else: \n",
    "                    lista= None\n",
    "                    score = None\n",
    "                tempo = re.findall(r'faceplate-timeago ts=\".*?\"',str(times[i]))\n",
    "                if len(tempo)>0:\n",
    "                    tempo=tempo[0]\n",
    "                    tempo=re.sub(r'faceplate-timeago ts=','',tempo)\n",
    "                    tempo=re.sub(r'\"','',tempo)\n",
    "                    data_hora_objeto = datetime.strptime(tempo, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "                    data_hora_formatada = data_hora_objeto.strftime(\"%Y:%m:%d %H:%M:%S\")\n",
    "                else: data_hora_formatada= None\n",
    "                dici_post['title']=titulo\n",
    "                dici_post['subreddit']=subredit\n",
    "                dici_post['comments']=lista\n",
    "                dici_post['score']=score\n",
    "                dici_post['time']=data_hora_formatada\n",
    "                lista_b.append(dici_post)\n",
    "            json_string = json.dumps(lista_b,ensure_ascii=False, indent=2)\n",
    "            print(json_string)\n",
    "            return json_string\n",
    "        else: \n",
    "            print('Tópico não encontrado')\n",
    "            return None\n",
    "    else: print('Soup Is NONE.')\n",
    "            \n",
    "\n",
    "def f_categoria(categoria):\n",
    "    lista_cat=[]\n",
    "    urlx='https://www.reddit.com/'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(urlx)\n",
    "    time.sleep(4)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    driver.quit()\n",
    "    topicos=soup.find('faceplate-auto-height-animator', class_='block')\n",
    "    cada=topicos.find_all('left-nav-topic-tracker', noun='topic_item')\n",
    "    for i in cada:\n",
    "        t=i.find('span', class_='flex flex-col justify-center min-w-0 shrink py-[var(--rem6)]')\n",
    "        t=t.text.strip()\n",
    "        lista_cat.append(t.lower())\n",
    "    palavra_proxima = get_close_matches(categoria.lower(), lista_cat)\n",
    "    if palavra_proxima:\n",
    "        topico=palavra_proxima[0]\n",
    "        cat=f'https://www.reddit.com/search/?q={topico}&sort=new'\n",
    "        print(cat)\n",
    "        return cat\n",
    "    else:\n",
    "        return None \n",
    "# #f_categoria('nba')\n",
    "# alinea_b(2,'nba')\n",
    "\n",
    "def algoritmo_c(limite, categoria):\n",
    "    resultado_alinea_b = alinea_b(limite,categoria)\n",
    "    try:\n",
    "        resultado_alinea_b = json.loads(resultado_alinea_b)\n",
    "        if resultado_alinea_b:\n",
    "            data_atual = datetime.now()\n",
    "            for post in range(len(resultado_alinea_b)):\n",
    "                score = resultado_alinea_b[post].get('score', 'N/A') # score\n",
    "                comment=resultado_alinea_b[post].get('comments', 'N/A')\n",
    "                time=resultado_alinea_b[post].get('time', 'N/A')\n",
    "                if time is not None:\n",
    "                    time = datetime.strptime(time, '%Y:%m:%d %H:%M:%S')\n",
    "                    dif_tempo= abs((data_atual - time).total_seconds())\n",
    "                else: dif_tempo = 100000000000000000\n",
    "                #print(dif_tempo)\n",
    "                n_comment=len(comment) # num de comentarios\n",
    "                num_resposta=0\n",
    "                if n_comment>0:\n",
    "                    for k in comment:\n",
    "                         if isinstance(k, str):\n",
    "                            resposta = k.get('resposta')\n",
    "                            if resposta:\n",
    "                                num_resposta+=len(resposta) # quantas respostas para cada comentario\n",
    "\n",
    "                resultado_alinea_b[post]['relevancia'] = (0.5 * int(score) if score is not None else 0 +\n",
    "                                          0.4 * n_comment + 0.1 * num_resposta) + 100\n",
    "                #print(resultado_alinea_b[post]['relevancia'])\n",
    "                while dif_tempo > 60: # mais de 1 minuto\n",
    "                    dif_tempo= dif_tempo-60\n",
    "                    resultado_alinea_b[post]['relevancia']= resultado_alinea_b[post]['relevancia']-1\n",
    "            json_relevancia =sorted(resultado_alinea_b, key=lambda x: x.get('relevancia', 0), reverse=True)\n",
    "            json_relevancia=json.dumps(json_relevancia, indent=2)\n",
    "            print(json_relevancia)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"A função alinea_b retornou uma string inválida.\")\n",
    "\n",
    "    qst=str(input(f'Deseja comparar os resultados com os mais populares da categoria {categoria} ?'))\n",
    "\n",
    "    while qst != 's' and qst != 'n':\n",
    "        qst=str(input('Insira (s) para sim ou (n) para não, em minúsculo: '))\n",
    "\n",
    "    if qst == 's':\n",
    "        alinea_d(limite, categoria)\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# função que pega n posts mais populares da categoria popular (alinea a: (top(N)) ou e: (top(N,include_score=True)) ) ou de uma dada categoria (uso na alinea d)\n",
    "def top(num: int, include_score = False, url_d = 'https://www.reddit.com/r/popular/'):\n",
    "    top={}\n",
    "    soup = carregar_mais_posts(20, url_d)\n",
    "    if soup:\n",
    "        posts=soup.find('shreddit-feed',class_='nd:visible')\n",
    "        posts_ind=posts.find_all('article',class_=\"m-0\")\n",
    "        n=0\n",
    "        if len(posts_ind)==num: #mudar para < depois\n",
    "            print('menor')\n",
    "        else:\n",
    "            for p in posts_ind:\n",
    "                if n<num:\n",
    "                    info=p.find('shreddit-post')\n",
    "                    titulo=info['post-title']\n",
    "                    subreddit=info['subreddit-prefixed-name']\n",
    "                    score=info['score']\n",
    "                    coment=info['comment-count']\n",
    "                    n+=1\n",
    "                    if include_score:\n",
    "                        base_url = \"https://www.reddit.com\"\n",
    "                        pl = p.find('a', slot='full-post-link')\n",
    "                        post_link = pl['href']\n",
    "                        if not re.match(r'^https://www\\.reddit\\.com', post_link):                            \n",
    "                            url = urljoin(base_url, post_link)\n",
    "                        else:\n",
    "                            url = post_link\n",
    "                        print(url)\n",
    "                        comment_w_score, scor = extract_comments(url)\n",
    "                        if comment_w_score != []:\n",
    "\n",
    "                            sorted_list = sorted(comment_w_score, key=lambda x: int(x['score']), reverse=True)\n",
    "                            sort_n = sorted_list[:5]\n",
    "                            # sort_n = json.dumps(sorted_list,ensure_ascii=False, indent=2)\n",
    "                            top[n]={'titulo':titulo,\n",
    "                                'subreddit':subreddit,\n",
    "                                'score POST':score,\n",
    "                                'comment':sort_n,\n",
    "                                }\n",
    "                        else:\n",
    "                            top[n]={'titulo':titulo,\n",
    "                                'subreddit':subreddit,\n",
    "                                'score POST':score,\n",
    "                                'comment': 'indisponível',\n",
    "                                }\n",
    "                    else: \n",
    "                        top[n]={'titulo':titulo,\n",
    "                            'subreddit':subreddit,\n",
    "                            'score POST':score,\n",
    "                            'comment':coment}\n",
    "                        \n",
    "                else:\n",
    "                    break\n",
    "        print(json.dumps(top,ensure_ascii=False, indent=2))\n",
    "        csv=str(input('Deseja inportar para formato csv(s/n)? '))\n",
    "        while csv != 's' and csv != 'n':\n",
    "            csv=str(input('Insira (s) para sim ou (n) para não, minúsculo: '))\n",
    "        if csv == 's':\n",
    "            csv_(top)\n",
    "    else:\n",
    "        print(f'Falha ao obter a página.')\n",
    "    return json.dumps(top,ensure_ascii=False, indent=2)\n",
    "\n",
    "# função que é usada como opção para comparar posts mais relevantes com posts mais recentes a partir da função algoritmo_c\n",
    "def alinea_d(limite, categoria):\n",
    "    url_d = f'https://www.reddit.com/search/?q={categoria}&sort=hot'\n",
    "    # posts_cat = carregar_mais_posts(limite, url_d)\n",
    "    # print(posts_cat)\n",
    "    resk = top(limite, url_d)\n",
    "       \n",
    "\n",
    "algoritmo_c(2, 'nba')\n",
    "# top(3, include_score=True)\n",
    "\n",
    "#criação de aplicativo de linha de comando\n",
    "app=typer.Typer()\n",
    "#para que o cliente use a função\n",
    "@app.command()\n",
    "\n",
    "\n",
    "def teste(nome: str):\n",
    "    print(nome)\n",
    "\n",
    "#correr o typer\n",
    "if __name__=='__main__':\n",
    "    app()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
